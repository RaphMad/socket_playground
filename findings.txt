* Shutdown protocol / "early FIN"
* Windows stack sends RST when closing with incoming data (as well as on lots of other cases, e.g. sent unacked data?)
* Sometimes referred to as "half-closed"


* Block vs. non-block:
** also depends on WiFi vs. LAN

** WLAN
** block: most of the time reading chunks of 64k or 500k, only reaching about 50mbps, TCP window gets exhausted (although sometimes the window gets increased to a better value)
** nonblock:  most of the time reading chunks of 1460 - 16k, reaching 200mbps (network speed), TCP window never gets exhausted (and it seems that a window size increase kicks in a lot faster, even though its unneeded)

** LAN
** block: single big receive (2g chunk), but only when wireshark is _not_ running, sometimes also bad performance when wireshark is running
** nonblock: wireshark seems to influence the result (OS level hangup), chunks from 1460 to about 20k ("small - big" pattern)

* general - for some "magical" reason windows manages to arrive at the maximum throughput without packet drops (at least when testing in LAN)
* some congestion avoidance (packet drops, then send rate is adjusted) can be seen when sender is in LAN, receiver is in WLAN -> there surely is some "carrier sensing" going on with senders, but its still strange it is so good at guessing a dynamic WLANs speed
* Seems like I just confused sender/reciver - of course congestion handling is done by sender -> Shows nice linear growth until packet loss occurs and speed stabilizes, but again sometimes there are minor drops, but not when reaching max bandwidth
* reading in a nonblocking way seems to be "better" for the TCP algorithms, reaches max bandwidth regardless of wireshark/system load

* Buffers
** It seems that send buffer is not really limited, even 2GB can be put in there atomically
** Sending app can even terminate early, FINS will still be sent as expected
** Receive buffer is actually receive window - it is defined as "max bytes that can be sent unacknowledged",
but its also the acknowledge that updates the window size. So when no bytes are read by the application,
sender will receive an ACK but will still indicate window size 0 - "0 bytes can be sent unacknowledged now".

** Blocking code:
*** Packets are still dripping in at target while sending app was closed
*** Memory shows up in receiver at taskmanager (so where "are" the packets? according to wireshark the sender still has them, but memory shows up at the receiver)

** Nonblocking code:
*** Packets are still dripping in at target while sending app was closed
*** Memory shows a more "sane" value (at sender high while sending, at receiver going up while receiving)

* Wireshark / Layer 1 / Layer 2:
** Layer 1 ethernet packets have preamble (7 bytes), start frame delimiter (1 byte) and inter packet gap (12 bytes)
** Wireshark does not show those
** Layer 2 ethernet frames have the standard header stuff (14 byte), shown by wireshark + 4 byte check sequence (not shown by wireshark)
** When a network device detects a corrupt FCS, the frame is simply discarded
** Layer 3 TCP also does a checksum, re-transmits packets if checksum fails
** But in general, higher level checksums should be applied to catch errors that occur "before" data is passed to the network https://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html

* Framing
** TCP provides no concept of "complete" messages, its just a data stream. So protocols/applications need to use mechanisms like "message end markers" (computationally intensive to detect them mid-message, but HTTP seems to do this with double newline) or message-length headers (WS seems to do this - https://tools.ietf.org/html/rfc6455#section-5.2) to allow receivers to reconstruct messages.
** UDP is packet-oriented up to a max size, so either apply the above mechanisms within single packets, or define an application-level protocol spanning multiple packets

* Keepalive
** Can be activated via socket option, 2 hours default time
** Visible in Wireshark when both clients are up
** Invisible in Wireshark when one client has gone missing - still get an aplication level event (why?)

* Nagling
** Is better for the network (less packets) and in most cases even overall (effective bandwidth) when transmitting lots of small packets in quick succession (first packet is sent, waiting for first ack - meanwhile lots of packets can pile up and will be sent in a burst).
** In the test environment, the only positive effect is getting a better latency for packet 2 when disabling nagling (but at what cost?).

* Socket options
** Send/Recieve window size is reported as 64k, but in fact seems to be a lot higher (verified by maximum atomic send parameter + receive window in wireshark)

* PSH flag is set only for the last packet, when data > MSS is sent via a single call

* UDP can flood the network, but it seems this needs multiple instances to reach limits
* limits are higher than TCP (as expected)
* Detecting packet loss is actually hard in such scenarios (would need something like numbers counting up)
* Windows built-in speed tool (task manager) shows reached speeds quite nicely
** can reach up to 1gbps via wire, but console output is the bottleneck in that case (need to close conhost but keep app running), also 3-4 instances are needed


- experiment with half-open scenarios - possible via raw sockets?
- manually implement tls?

-- try to get an overview over the scenarios:

windows

blocking
non-blocking
async
wsa event based
overlapped i/o manual
overlapped i/o + completion ports

unix
blocking
non-blocking
async
